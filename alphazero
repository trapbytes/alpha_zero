#!//usr/bin/python3
#
#


import sys
import io
import math
import numpy as np
import os
from optparse import OptionParser
import random
import time
import traceback
import torch
import torch.nn as nn
import torch.nn.functional as F

torch.manual_seed(0)

from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
from tqdm import tqdm

#
import games
from nnmodules import ResNet



''' Node class to save state for Monte Carlo Tree Search '''
class Node:
    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):
        self.game = game
        self.args = args 
        self.state = state
        self.parent = parent
        self.action_taken = action_taken # player action
        self.prior = prior   # policy of a given action from the parents perspective

        self.children = []
        self.expandable_moves = game.get_valid_moves(state)

        self.visit_count = visit_count
        self.value_sum = 0

    def is_fully_expanded(self):
        return len(self.children) > 0

    def select(self):
        best_child = None
        best_ucb = -np.inf
        for child in self.children:
            ucb = self.get_ucb(child)
            if ucb > best_ucb:
               best_child = child
               best_ucb = ucb
        return best_child

    def get_ucb(self, child):
        if child.visit_count == 0:
           q_value = 0
        else:
           q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2
        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior

    def expand(self, policy):
        for action, prob in enumerate(policy):
            if prob > 0:
               child_state = self.state.copy()
               child_state = self.game.get_next_state(child_state, action, 1)
               child_state = self.game.change_perspective(child_state, player=-1)
        
               child = Node(self.game, self.args, child_state, self, action, prob)
               self.children.append( child )

        return child

    def simulate(self):
        value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)
        value = self.game.get_opponent_value(value)
        if is_terminal:
           return value
        rollout_state  = self.state.copy()
        rollout_player = 1
        while True:
            valid_moves = self.game.get_valid_moves(rollout_state)
            action = np.random.choice(np.where(valid_moves == 1)[0])
            rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)
            value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)
            if is_terminal:
               if rollout_player == -1:
                  value = self.game.get_opponent_value(value)
               return value
            # flip player around
            rollout_player = self.game.get_opponent(rollout_player)

    def backpropagate(self, value):
        self.value_sum += value
        self.visit_count += 1 
        value = self.game.get_opponent_value(value)
        if self.parent is not None:
           self.parent.backpropagate(value)


''' Monte Carlo Tree Search '''
class MCTS:
    def __init__(self, game, args, model):
        self.game = game
        self.args = args
        self.model = model

    @torch.no_grad()
    def search(self, state):
        # define root node 
        root = Node(self.game, self.args, state, visit_count=1)
        # add some noise 
        policy, _ = self.model(
               torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)
        )
        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()

        # add some randomness to the policy
        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \
                  * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)

        valid_moves = self.game.get_valid_moves(state)
        policy *= valid_moves
        policy /= np.sum(policy)
        #
        root.expand(policy)

        for search in range(self.args['num_searches']):
            node = root

            while node.is_fully_expanded():  # selection
                node = node.select()

            # check if node is a terminal node   
            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)
            value = self.game.get_opponent_value(value)

            if not is_terminal:
               policy, value = self.model(
                       torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)
               )
               policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()
               # mask out the illegal moves
               valid_moves = self.game.get_valid_moves(node.state)
               policy *= valid_moves
               policy /= np.sum(policy) 
               value = value.item()

               node.expand(policy)    # expansion
            #
            node.backpropagate(value) # backpropagation
        
        # return visit counts
        action_probs = np.zeros(self.game.action_size)
        for child in root.children:
            action_probs[child.action_taken] = child.visit_count
        action_probs /= np.sum(action_probs)
        return action_probs


class AlphaZero:
    def __init__(self, model, optimizer, game, args):
        self.model = model
        self.optimizer = optimizer
        self.game = game
        self.args = args
        self.mcts = MCTS(game, args, model)
    
    ''' self-play loop where we iterate and adjust, fine tune the model 
        return a list of tuples from the self play iteration
    '''
    def selfPlay(self):
        memory = []
        player = 1
        state = self.game.get_initial_state()
        while True:
           neutral_state = self.game.change_perspective(state, player)
           action_probs = self.mcts.search(neutral_state)

           memory.append((neutral_state, action_probs, player))

           # tweak the distribution
           temperature_action_probs = action_probs ** ( 1 / self.args['temperature'])
           temperature_action_probs /= np.sum(temperature_action_probs) 
           action = np.random.choice(self.game.action_size, p=temperature_action_probs)

           state = self.game.get_next_state(state, action, player)

           value, is_terminal = self.game.get_value_and_terminated(state, action)
           if is_terminal:
              returnMemory = []
              for hist_neutral_state, hist_action_probs, hist_player in memory:
                  #
                  hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)
                  returnMemory.append((
                     self.game.get_encoded_state(hist_neutral_state),
                     hist_action_probs,
                     hist_outcome
                  ))
              return returnMemory
           #
           palyer = self.game.get_opponent(player)

    ''' train our model based on the self-play data '''
    def train(self, memory):
        # shuffle our training data
        random.shuffle(memory)

        for batchIdx in tqdm(range(0, len(memory), self.args['batch_size'])):
            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]

            # get the state mcts probs etc 
            state, policy_targets, value_targets = zip(*sample)  # transpose our sample

            # reshape value targets so each in its own subarray
            state, policy_targets, value_targets = np.array(state),np.array(policy_targets),np.array(value_targets).reshape(-1,1)

            # transform into tensors
            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)
            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)
            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)

            #get the out policy and the out value from our model
            out_policy, out_value = self.model(state)

            # run cross entropy and mse loss
            policy_loss = F.cross_entropy(out_policy, policy_targets)
            value_loss = F.mse_loss(out_value, value_targets)

            # sum of both loss values
            loss = policy_loss + value_loss

            # minimize by back propagating
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()  # pytorch dose all the backpropagation for us

    def learn(self):
        for iteration in range(self.args['num_iterations']):
            memory = []

            self.model.eval(); # eval mode for the model
            for selfPlay_iteration in tqdm(range(self.args['num_selfPlay_iterations'])):
                memory += self.selfPlay()
        
            self.model.train()
            for epoch in tqdm(range(self.args['num_epochs'])):
                self.train(memory)
            torch.save( self.model.state_dict(), f"data-alpha/model_{iteration}_{self.game}.pt")
            torch.save( self.optimizer.state_dict(), f"data-alpha/optimizer_{iteration}_{self.game}.pt")



class SPG:  # self play game used to parallelize the training
    def __init__(self, game):
        self.state = game.get_initial_state()
        self.memory = []
        self.root = None
        self.node = None



class MCTSParallel:
    def __init__(self, game, args, model):
        self.game = game
        self.args = args
        self.model = model

    @torch.no_grad()
    def search(self, states, spGames):
        # 
        policy, _ = self.model(
               torch.tensor(self.game.get_encoded_state(states), device=self.model.device)
        )
        policy = torch.softmax(policy, axis=1).cpu().numpy()
        # add randomness to the policy
        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \
                  * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])

        for i, spg in enumerate(spGames):
            spg_policy = policy[i]
            valid_moves = self.game.get_valid_moves(states[i])
            spg_policy *= valid_moves
            spg_policy /= np.sum(spg_policy)
            #
            # define root node
            spg.root = Node(self.game, self.args, states[i], visit_count=1)
            spg.root.expand(spg_policy)

        for search in range(self.args['num_searches']):
            for spg in spGames:
                spg.node = None
                node = spg.root

                while node.is_fully_expanded():  # selection
                    node = node.select()

                # check if node is a terminal node
                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)
                value = self.game.get_opponent_value(value)

                if is_terminal:
                   node.backpropagate(value) # backpropagation
                else:
                   spg.node = node

            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]

            if len(expandable_spGames) > 0:
               states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])

               policy, value = self.model(
                       torch.tensor(self.game.get_encoded_state(states), device=self.model.device)
               )
               policy = torch.softmax(policy, axis=1).cpu().numpy()

            for i, mappingIdx in enumerate(expandable_spGames):
                node = spGames[mappingIdx].node
                spg_policy, spg_value = policy[i], value[i]

                valid_moves = self.game.get_valid_moves(node.state)
                spg_policy *= valid_moves
                spg_policy /= np.sum(spg_policy)

                node.expand(spg_policy)       # expansion
                node.backpropagate(spg_value) # backpropagation



class AlphaZeroParallel:
    def __init__(self, model, optimizer, game, args):
        self.model = model
        self.optimizer = optimizer
        self.game = game
        self.args = args
        self.mcts = MCTSParallel(game, args, model)

    ''' self-play loop where we iterate and adjust, fine tune the model
        return a list of tuples from the self play iteration
    '''
    def selfPlay(self):
        return_memory = []
        player = 1

        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]

        while len(spGames) > 0:
           #
           states = np.stack([spg.state for spg in spGames])
           
           neutral_states = self.game.change_perspective(states, player)
           self.mcts.search(neutral_states, spGames)

           for i in range(len(spGames))[::-1]:
               # remove from list if they are terminal
               spg = spGames[i]   
               # 
               action_probs = np.zeros(self.game.action_size)
               for child in spg.root.children:
                   action_probs[child.action_taken] = child.visit_count
               action_probs /= np.sum(action_probs)
  
               spg.memory.append((spg.root.state, action_probs, player))

               # tweak the distribution
               temperature_action_probs = action_probs ** ( 1 / self.args['temperature'])
               temperature_action_probs /= np.sum(temperature_action_probs)

               action = np.random.choice(self.game.action_size, p=temperature_action_probs)

               spg.state = self.game.get_next_state(spg.state, action, player)

               value, is_terminal = self.game.get_value_and_terminated(spg.state, action)
               if is_terminal:
                  for hist_neutral_state, hist_action_probs, hist_player in spg.memory:
                      #
                      hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)
                      return_memory.append((
                         self.game.get_encoded_state(hist_neutral_state),
                         hist_action_probs,
                         hist_outcome
                      ))
                  del spGames[i]
           #
           palyer = self.game.get_opponent(player)
        return return_memory

    ''' train our model based on the self-play data '''
    def train(self, memory):
        # shuffle our training data
        random.shuffle(memory)

        for batchIdx in tqdm(range(0, len(memory), self.args['batch_size'])):

            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]

            # get the state mcts probs etc
            state, policy_targets, value_targets = zip(*sample)  # transpose our sample

            # reshape value targets so each in its own subarray
            state, policy_targets, value_targets = \
                   np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1,1)

            # transform into tensors
            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)
            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)
            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)

            #get the out policy and the out value from our model
            out_policy, out_value = self.model(state)

            policy_loss = F.cross_entropy(out_policy, policy_targets)
            value_loss = F.mse_loss(out_value, value_targets)

            # sum of both loss values
            loss = policy_loss + value_loss

            # minimize by back propagating
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()  # pytorch dose all the backpropagation for us

    def learn(self):
        for iteration in range(self.args['num_iterations']):
            memory = []

            self.model.eval(); # eval mode for the model
            for selfPlay_iteration in tqdm(range(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games'])):
                memory += self.selfPlay()

            self.model.train()
            #
            for epoch in tqdm(range(self.args['num_epochs'])):
                self.train(memory)
            torch.save( self.model.state_dict(), f"data-alpha/model_{iteration}_{self.game}.pt")
            torch.save( self.optimizer.state_dict(), f"data-alpha/optimizer_{iteration}_{self.game}.pt")


def set_torch_device():
    return (
            "cuda"
            if torch.cuda.is_available()
            else "mps"
            if torch.backends.mps.is_available()
            else "cpu"
           )


''' play the game vs yourself validate that the game works logically '''
def game_no_mtcs():
    game = games.TicTacToe()
    player = 1
    state = game.get_initial_state()
    while True:
         print(f'state: {state}')
         valid_moves = game.get_valid_moves(state)
         print("valid moves", [i for i in range(game.action_size) if valid_moves[i] == 1])
         action = int(input(f"{player}:"))
         print(f'Action: {action}')

         if valid_moves[action] == 0:
            print("action not valid")
            continue

         state = game.get_next_state(state, action, player)
         value, is_terminal = game.get_value_and_terminated( state, action)
         if is_terminal:
            print(f'state: {state}')
            if value == 1:
               print(player, " won")
            else:
               print("draw")
            break
         player = game.get_opponent(player)


'''
  Here we set some initial state and see how the model interacts with it
'''
def test_run_saved_model_tictactoe():
    device = set_torch_device()
    # make a prediction based on some intial input moves
    game = games.TicTacToe()
    state = game.get_initial_state()

    # set our test states
    state = game.get_next_state(state, 2, -1)
    state = game.get_next_state(state, 4, -1)
    state = game.get_next_state(state, 6, 1)
    state = game.get_next_state(state, 8, 1)

    encoded_state = game.get_encoded_state(state)
    tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)

    model = ResNet(game, 4, 64, device)
    model.load_state_dict( torch.load(f'data-alpha/model_2_{game}.pt', map_location=device) )
    model.eval()

    policy, value = model(tensor_state)
    value = value.item()
    policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()
    print(f'value:\n{value}\nstate:\n{state}\ntensor_state:\n{tensor_state}')
    print(f'model_new_policy:\n{policy}')



''' test tictacto with a model that has not been trained '''
def test_mcts_tictactoe():
    game = games.TicTacToe()
    player = 1
    args = {
           'C': 2,
           'num_searches': 1000
    }
    model = ResNet(game, 4, 64)
    model.eval()
    mcts = MCTS(game, args, model)

    state = game.get_initial_state()

    while True:
        print(state)
        if player == 1:
           valid_moves = game.get_valid_moves(state)
           print("valid moves", [i for i in range(game.action_size) if valid_moves[i] == 1])
           action = int(input(f"{player}:"))
           print(f'Action: {action}')

           if valid_moves[action] == 0:
              print("action not valid")
              continue
        else:
           neutral_state = game.change_perspective(state, player)
           mcts_probs = mcts.search(neutral_state)
           action = np.argmax(mcts_probs)

        state = game.get_next_state(state, action, player)
        value, is_terminal = game.get_value_and_terminated( state, action)
        if is_terminal:
           print(state)
           if value == 1:
              print(player, " won")
           else:
              print("draw")
           break
        player = game.get_opponent(player)


''' test ConnectFour with a model that has not been trained '''
def test_mcts_connect_four():
    game = games.ConnectFour()
    device = set_torch_device()
    player = 1
    args = {'C': 2,
            'num_searches': 100,
            'dirichlet_epsilon': 0.0, # dont use noise
            'dirichlet_alpha': 0.3
    }

    model = ResNet(game, 9, 128, device)
    model.eval()
    mcts = MCTS(game, args, model)
    state = game.get_initial_state()
    while True:
        print(state)
        if player == 1:
           valid_moves = game.get_valid_moves(state)
           print("valid moves", [i for i in range(game.action_size) if valid_moves[i] == 1])
           action = int(input(f"{player}:"))
           print(f'Action: {action}')

           if valid_moves[action] == 0:
              print("action not valid")
              continue
        else:
           neutral_state = game.change_perspective(state, player)
           mcts_probs = mcts.search(neutral_state)
           action = np.argmax(mcts_probs)

        state = game.get_next_state(state, action, player)
        value, is_terminal = game.get_value_and_terminated( state, action)
        if is_terminal:
           print(state)
           if value == 1:
              print(player, " won")
           else:
              print("draw")
           break
        player = game.get_opponent(player)


'''
  Alpha-Zero TicTacToe
'''
def train_generate_alphazero_model():
    game = games.TicTacToe()
    model = ResNet(game, 4, 64, set_torch_device())
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
    args = {'C': 2,
            'num_searches': 60,
            'num_iterations': 3,
            'num_selfPlay_iterations': 500,
            'num_parallel_games': 100,
            'num_epochs': 4,
            'batch_size': 64,
            'temperature': 1.25,
            'dirichlet_epsilon': 0.25,
            'dirichlet_alpha': 0.3
           }
    alphazero = AlphaZero(model, optimizer, game, args)
    alphazero.learn()


'''
  Alpha-Zero ConnectFour
  We use the Parallel Implementation here to optimize the training
'''
def train_generate_alpha_connect_four():
    game = games.ConnectFour()
    model = ResNet(game, 9, 128, set_torch_device())
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
    args = {'C': 2,
            'num_searches': 600,
            'num_iterations':  8,
            'num_selfPlay_iterations': 500,
            'num_parallel_games': 100,
            'num_epochs': 4,
            'batch_size': 128,
            'dirichlet_epsilon': 0.25, 
            'dirichlet_alpha': 0.3
    }
    alphazero = AlphaZeroParallel(model, optimizer, game, args)
    alphazero.learn()


'''
  Test against a model that was trained
'''
def test_run_saved_model_connect4():
    device = set_torch_device()
    # make a prediction based on some intial input moves
    game = games.ConnectFour()
    player = 1
    args = {'C': 2,
            'num_searches': 100,
            'dirichlet_epsilon': 0.0,
            'dirichlet_alpha': 0.3
    }

    model = ResNet(game, 9, 128, device)
    model.load_state_dict( torch.load(f'data-alpha/model_6_{game}.pt', map_location=device) )
    model.eval()

    mcts = MCTS(game, args, model)
    state = game.get_initial_state()
    while True:
        print(state)
        if player == 1:
           valid_moves = game.get_valid_moves(state)
           print("valid moves", [i for i in range(game.action_size) if valid_moves[i] == 1])
           action = int(input(f"{player}:"))
           try:
             if valid_moves[action] == 0:
                print("action not valid")
                continue
           except:
              print("action not valid")
              continue
           print(f'Action: {action}')
        else:
           neutral_state = game.change_perspective(state, player)
           mcts_probs = mcts.search(neutral_state)
           action = np.argmax(mcts_probs)

        state = game.get_next_state(state, action, player)
        value, is_terminal = game.get_value_and_terminated( state, action)
        if is_terminal:
           print(state)
           if value == 1:
              print(player, " won")
           else:
              print("draw")
           break
        player = game.get_opponent(player)



def parse_options():
    p = OptionParser()
    p.add_option("", "--test-model", dest="test_model", \
                 default=False, action="store_true", \
                 help="Test a trained model in a game")
    p.add_option("", "--test-untrained-model", dest="test_untrained_model", \
                 default=False, action="store_true", \
                 help="Test an un-trained model [MTCS only] in a game")


    p.add_option("", "--model-game", dest="model_game", \
                 default=None, \
                 help="Which game model to choose 'TicTacToe' or 'ConnectFour")

    p.add_option("", "--human-only", dest="human_only", \
                 default=False, action="store_true", \
                 help="Play yourself in a game :) ")

    p.add_option("", "--train-model", dest="train_model_game", \
                 default=False, action="store_true", \
                 help="Which game model to choose 'TicTacToe' or 'ConnectFour")

     
    (opts, _args) = p.parse_args()
    #
    if not opts.test_model and not opts.test_untrained_model \
       and not opts.human_only and not opts.train_model_game:
       p.error(" no valid options specified")
    if not opts.model_game:
       p.error(" not valid option of --model-game  TicTacToe|ConnectFour")
   
    return opts 


if __name__ == '__main__':

   opts = parse_options()

   try:
      ''' train models '''
      if opts.train_model_game:
         if opts.model_game == 'TicTacToe':
            train_generate_alphazero_model()
         else:
            train_generate_alpha_connect_four()

      ''' test models that we generated '''
      if opts.test_model:
         if opts.model_game == 'TicTacToe':
            test_run_saved_model_tictactoe()
         else:
            test_run_saved_model_connect4()

      ''' play game to validate gmae logic '''
      if opts.human_only:
         if opts.model_game == 'TicTacToe':
            game_no_mtcs()
         else:
            print(f'Unsupported.. self play {opts.model_game}')
       
      ''' test games with only MCTS '''   
      if opts.test_untrained_model:
         if opts.model_game == 'TicTacToe':
            test_mcts_tictactoe()
         else:
            test_mcts_connect_four()

      sys.exit(0)
   except Exception as er:
      print(traceback.format_exc())
      sys.exit(1)
